{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/Documents/repos/rust-programming/rust-rag/.pixi/envs/default/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import  SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    "    RetrieverEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True)\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=60, temperature=0)\n",
    "# qwen2 = Ollama(model=\"qwen2.5:latest\", request_timeout=60)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided. Your response should include \\\n",
    "the questions separated by a newline and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_from_index(index):\n",
    "    \"\"\"Gets the nodes from the index\"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=99999999999)\n",
    "    all_nodes = retriever.retrieve(\"dummy\")\n",
    "    all_nodes = [item.node for item in all_nodes]\n",
    "    return all_nodes\n",
    "\n",
    "\n",
    "def build_index(documents, embed_model=embed_model or Settings.embed_model, db_path=\"../chromadb\", collection_name=\"rust_book\", rebuild=False):\n",
    "    \"\"\"Builds the index\"\"\"\n",
    "    db = chromadb.PersistentClient(db_path)\n",
    "    collection = db.get_or_create_collection(collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    if collection.count() > 0 and not rebuild:\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n",
    "    else:\n",
    "        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "    return db, collection, vector_store, index\n",
    "\n",
    "\n",
    "def display_results(name, eval_results, metrics):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we want to compare different storage / embedding methods, we need to rebuild the index and qa-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 490, updating n_results = 490\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('../txt').load_data()\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:23<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# n_nodes = 100\n",
    "# qa_dataset = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset.save_json(\"../data/qa_dataset.json\"),\n",
    "\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def evaluate_retriever(retriever, qa_dataset=qa_dataset, metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]):\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        metrics, retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "    return  display_results(\"baseline top-2 eval\", eval_results, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.08792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate     mrr  precision  recall      ap     ndcg\n",
       "0  baseline top-2 eval      0.16  0.1375       0.08    0.16  0.1375  0.08792"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await evaluate_retriever(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.1525</td>\n",
       "      <td>0.097118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate     mrr  precision  recall      ap      ndcg\n",
       "0  baseline top-2 eval     0.175  0.1525     0.0875   0.175  0.1525  0.097118"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)\n",
    "query_fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(), bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=2,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "await evaluate_retriever(query_fusion_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improvement, but nothing noteworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a different embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 490, updating n_results = 490\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\", trust_remote_code=True)\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents, embed_model=embed_model, collection_name=\"rust-rag-all-miniLM-L12-v2\", rebuild=True)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate  mrr  precision  recall   ap  ndcg\n",
       "0  baseline top-2 eval       0.0  0.0        0.0     0.0  0.0   0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await evaluate_retriever(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the indexes are built differently, evaluation on the same dataset doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_dataset_all_mini = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset_all_mini.save_json(\"../data/qa_dataset_all_mini.json\")\n",
    "\n",
    "qa_dataset_all_mini = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_all_mini.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.09292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate    mrr  precision  recall     ap     ndcg\n",
       "0  baseline top-2 eval      0.17  0.145      0.085    0.17  0.145  0.09292"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await evaluate_retriever(retriever, qa_dataset=qa_dataset_all_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/GameScribes/stella_en_400M_v5:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/GameScribes/stella_en_400M_v5:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"GameScribes/stella_en_400M_v5\", trust_remote_code=True)\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents, embed_model=embed_model, collection_name=\"rust-rag-all-miniLM-L12-v2\", rebuild=True)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "qa_dataset_stella = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "qa_dataset_stella.save_json(\"../data/qa_dataset_stella.json\")\n",
    "\n",
    "qa_dataset_stella = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_stella.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_retriever(retriever, qa_dataset=qa_dataset_stella)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
