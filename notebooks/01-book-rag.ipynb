{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/carlos/Documents/repos/rust-programming/rust-rag/notebooks/mlruns/923516027696088727', creation_time=1731166672733, experiment_id='923516027696088727', last_update_time=1731166672733, lifecycle_stage='active', name='rust-book-rag', tags={}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow.experiments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import  SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    "    RetrieverEvaluator\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"rust-book-rag\")\n",
    "# experiment_id = .get_experiment_by_name(\"rust-book-rag\").experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True)\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=60, temperature=0)\n",
    "# qwen2 = Ollama(model=\"qwen2.5:latest\", request_timeout=60)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided. Your response should include \\\n",
    "the questions separated by a newline and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_from_index(index):\n",
    "    \"\"\"Gets the nodes from the index\"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=99999999999)\n",
    "    all_nodes = retriever.retrieve(\"dummy\")\n",
    "    all_nodes = [item.node for item in all_nodes]\n",
    "    return all_nodes\n",
    "\n",
    "\n",
    "def build_index(documents, embed_model=embed_model or Settings.embed_model, db_path=\"../chromadb\", collection_name=\"rust_book\", rebuild=False):\n",
    "    \"\"\"Builds the index\"\"\"\n",
    "    db = chromadb.PersistentClient(db_path)\n",
    "    collection = db.get_or_create_collection(collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    if collection.count() > 0 and not rebuild:\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n",
    "    else:\n",
    "        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "    return db, collection, vector_store, index\n",
    "\n",
    "\n",
    "def display_results(name, eval_results, metrics):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we want to compare different storage / embedding methods, we need to rebuild the index and qa-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 1470, updating n_results = 1470\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('../txt').load_data()\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:13<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# n_nodes = 150\n",
    "# qa_dataset = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset.save_json(\"../data/qa_dataset.json\"),\n",
    "\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_retriever(retriever, qa_dataset=qa_dataset, metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]):\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        metrics, retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "    return  display_results(\"baseline top-2 eval\", eval_results, metrics=metrics)\n",
    "\n",
    "async def log_retriever_eval(retriever, retriever_name, **kwargs):\n",
    "    with mlflow.start_run():\n",
    "        results = await evaluate_retriever(retriever, **kwargs)\n",
    "        mlflow.log_param(\"retriever\", retriever_name)\n",
    "        mlflow.log_metrics(*results.drop(columns=[\"retrievers\"]).to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "await log_retriever_eval(retriever, \"baseline top-2 eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)\n",
    "query_fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(), bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=2,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    results = await evaluate_retriever(retriever=query_fusion_retriever)\n",
    "    mlflow.log_param(\"retriever\", \"baseline query fusion\")\n",
    "    mlflow.log_metrics(*results.drop(columns=[\"retrievers\"]).to_dict(orient=\"records\"))\n",
    "\n",
    "# await log_retriever_eval(query_fusion_retriever, retriever_name=\"baseline query fusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight improvement, but nothing noteworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a different embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 980, updating n_results = 980\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\", trust_remote_code=True)\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents, embed_model=embed_model, collection_name=\"rust-rag-all-miniLM-L12-v2\", rebuild=True)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "await log_retriever_eval(retriever, retriever_name=\"all-mini-lm-l12-v2 retriever\")\n",
    "# await evaluate_retriever(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the indexes are built differently, evaluation on the same dataset doesn't make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [02:22<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 150\n",
    "qa_dataset_all_mini = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "qa_dataset_all_mini.save_json(\"../data/qa_dataset_all_mini.json\")\n",
    "\n",
    "qa_dataset_all_mini = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_all_mini.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_all_mini_eval = generate_question_context_pairs(nodes=nodes[n_nodes: n_nodes+50], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "qa_dataset_all_mini_eval.save_json(\"../data/qa_dataset_all_mini_eval.json\")\n",
    "\n",
    "qa_dataset_all_mini_eval = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_all_mini_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "await log_retriever_eval(retriever, retriever_name=\"all-mini-lm-l12-v2 retriever reindexed\", qa_dataset=qa_dataset_all_mini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.finetuning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformersFinetuneEngine\n\u001b[1;32m      3\u001b[0m ft_engine \u001b[38;5;241m=\u001b[39m SentenceTransformersFinetuneEngine(\n\u001b[1;32m      4\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mini-lm-l12-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model_output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_mini_lm_ft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     val_dataset\u001b[38;5;241m=\u001b[39mqa_dataset_all_mini_eval,\n\u001b[1;32m      7\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.finetuning'"
     ]
    }
   ],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "ft_engine = SentenceTransformersFinetuneEngine(\n",
    "    model_id=\"all-mini-lm-l12-v2\",\n",
    "    model_output_path=\"test_mini_lm_ft\",\n",
    "    val_dataset=qa_dataset_all_mini_eval,\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model = HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", trust_remote_code=True, )\n",
    "\n",
    "# db, collection, vector_store, index = build_index(documents, embed_model=embed_model, collection_name=\"rust-rag-stella\", rebuild=True)\n",
    "# nodes = get_nodes_from_index(index)\n",
    "\n",
    "# retriever = index.as_retriever(similarity_top_k=2)\n",
    "# query_engine = index.as_query_engine()\n",
    "\n",
    "# qa_dataset_stella = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset_stella.save_json(\"../data/qa_dataset_stella.json\")\n",
    "\n",
    "# qa_dataset_stella = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_stella.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluate_retriever(retriever, qa_dataset=qa_dataset_stella)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rust-rag-l9LsLaFj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
