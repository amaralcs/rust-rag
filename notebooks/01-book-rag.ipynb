{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/carlos/Documents/repos/rust-programming/rust-rag/notebooks/mlruns/923516027696088727', creation_time=1731166672733, experiment_id='923516027696088727', last_update_time=1731166672733, lifecycle_stage='active', name='rust-book-rag', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow.experiments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "import llama_index.core\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import  SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    "    RetrieverEvaluator\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.llama_index.autolog()\n",
    "\n",
    "mlflow.set_experiment(\"rust-book-rag\")\n",
    "# experiment_id = .get_experiment_by_name(\"rust-book-rag\").experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "# Look for a URL in the output to open the App in a browser.\n",
    "px.launch_app()\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnet_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\", trust_remote_code=True)\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=60, temperature=0)\n",
    "# qwen2 = Ollama(model=\"qwen2.5:latest\", request_timeout=60)\n",
    "\n",
    "Settings.embed_model = mpnet_embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided. Your response should include \\\n",
    "the questions separated by a newline and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_from_index(index):\n",
    "    \"\"\"Gets the nodes from the index\"\"\"\n",
    "    retriever = index.as_retriever(similarity_top_k=99999999999)\n",
    "    all_nodes = retriever.retrieve(\"dummy\")\n",
    "    all_nodes = [item.node for item in all_nodes]\n",
    "    return all_nodes\n",
    "\n",
    "\n",
    "def build_index(documents, embed_model=Settings.embed_model, db_path=\"../chromadb\", collection_name=\"rust_book\", rebuild=False, distance_fn=\"l2\"):\n",
    "    \"\"\"Builds the index\"\"\"\n",
    "\n",
    "    db = chromadb.PersistentClient(db_path)\n",
    "\n",
    "    if rebuild:\n",
    "        db.delete_collection(name=collection_name)\n",
    "\n",
    "    collection = db.get_or_create_collection(collection_name, metadata={\"hnsw:space\": distance_fn})\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    if collection.count() > 0 and not rebuild:\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n",
    "    else:\n",
    "        index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "    return db, collection, vector_store, index\n",
    "\n",
    "\n",
    "def display_results(name, eval_results, metrics, return_agg=True):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    if return_agg:\n",
    "        return metric_df\n",
    "    else:\n",
    "        return full_df, metric_df\n",
    "\n",
    "async def evaluate_retriever(retriever, qa_dataset, metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"]):\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        metrics, retriever=retriever\n",
    "    )\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "    return  display_results(\"baseline top-2 eval\", eval_results, metrics=metrics)\n",
    "\n",
    "\n",
    "async def log_retriever_eval(retriever, retriever_name, **kwargs):\n",
    "    with mlflow.start_run():\n",
    "        results = await evaluate_retriever(retriever, **kwargs)\n",
    "        mlflow.log_param(\"retriever\", retriever_name)\n",
    "        mlflow.log_metrics(*results.drop(columns=[\"retrievers\"]).to_dict(orient=\"records\"))\n",
    "    return results\n",
    "\n",
    "async def evaluate_embed_model(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    retriever_name=None,\n",
    "    top_k=2,\n",
    "):\n",
    "    \"\"\"Evaluates the embedding model on a given dataset.\"\"\"\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, embed_model=embed_model, show_progress=False\n",
    "    )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    retriever_name = retriever_name or embed_model.model_name\n",
    "    results = await log_retriever_eval(retriever,  qa_dataset=dataset, retriever_name=retriever_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If we want to compare different storage / embedding methods, we need to rebuild the index and qa-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 384, updating n_results = 384\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('../txt').load_data()\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate qa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_nodes = 150\n",
    "# qa_dataset = generate_question_context_pairs(nodes=nodes[:n_nodes], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset.save_json(\"../data/qa_dataset.json\"),\n",
    "\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7dee28b58c20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/carlos/.cache/pypoetry/virtualenvs/rust-rag-l9LsLaFj-py3.11/lib/python3.11/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/home/carlos/.cache/pypoetry/virtualenvs/rust-rag-l9LsLaFj-py3.11/lib/python3.11/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.576667</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.576667</td>\n",
       "      <td>0.364821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  baseline top-2 eval  0.646667  0.576667   0.323333  0.646667  0.576667   \n",
       "\n",
       "       ndcg  \n",
       "0  0.364821  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await evaluate_embed_model(qa_dataset, embed_model=mpnet_embed_model, retriever_name=\"mpnet top-2 eval\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=2)\n",
    "query_fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(), bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=1,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.388129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  baseline top-2 eval  0.716667  0.603333   0.358333  0.716667  0.603333   \n",
       "\n",
       "       ndcg  \n",
       "0  0.388129  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await log_retriever_eval(query_fusion_retriever, retriever_name=\"baseline query fusion\", qa_dataset=qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are better! Note that the precision will be at most 0.5 because we're always retrieving 2 documents while the qa-dataset has only 1 expected results per question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a different embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 99999999999 is greater than number of elements in index 384, updating n_results = 384\n"
     ]
    }
   ],
   "source": [
    "mini_lm_embed = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L12-v2\", trust_remote_code=True)\n",
    "\n",
    "db, collection, vector_store, index = build_index(documents, embed_model=mini_lm_embed, collection_name=\"rust-rag-all-miniLM-L12-v2\", rebuild=False)\n",
    "nodes = get_nodes_from_index(index)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=2)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.299321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate       mrr  precision  recall        ap  \\\n",
       "0  baseline top-2 eval      0.53  0.473333      0.265    0.53  0.473333   \n",
       "\n",
       "       ndcg  \n",
       "0  0.299321  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_lm_results = await evaluate_embed_model(qa_dataset, embed_model=mini_lm_embed, retriever_name=\"miniLM top-2 eval\", top_k=2)\n",
    "mini_lm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the embedding model makes a big difference! Let's try a couple more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.431196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate       mrr  precision  recall        ap  \\\n",
       "0  baseline top-2 eval      0.75  0.686667      0.375    0.75  0.686667   \n",
       "\n",
       "       ndcg  \n",
       "0  0.431196  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stella_small_embed = HuggingFaceEmbedding(model_name=\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)\n",
    "\n",
    "stella_results = await evaluate_embed_model(qa_dataset, embed_model=stella_small_embed, retriever_name=\"stella top-2 eval\", top_k=2)\n",
    "stella_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity, let's see how stella performs with a fusion query retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.47139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate   mrr  precision  recall    ap     ndcg\n",
       "0  baseline top-2 eval      0.85  0.74      0.425    0.85  0.74  0.47139"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = qa_dataset.corpus\n",
    "\n",
    "qa_nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "index = VectorStoreIndex(\n",
    "    qa_nodes, embed_model=stella_small_embed, show_progress=False\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=qa_nodes, similarity_top_k=2)\n",
    "stella_query_fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(), bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=1,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "results = await log_retriever_eval(stella_query_fusion_retriever,  qa_dataset=qa_dataset, retriever_name=\"stella query-fusion top-2\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's test one more embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline top-2 eval</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.378333</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.43453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            retrievers  hit_rate       mrr  precision    recall        ap  \\\n",
       "0  baseline top-2 eval  0.756667  0.691667   0.378333  0.756667  0.691667   \n",
       "\n",
       "      ndcg  \n",
       "0  0.43453  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_large_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", trust_remote_code=False)\n",
    "\n",
    "bge_large_results = await evaluate_embed_model(qa_dataset, embed_model=bge_large_embed, retriever_name=\"bge-large top-2 eval\", top_k=2)\n",
    "bge_large_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fine tune these models and see which one gives better performance. Let's try first with MiniLM as it is a small model and should be 'quick enough'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307 77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_nodes, test_nodes = train_test_split(nodes, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/77 [00:02<02:44,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/77 [00:03<02:13,  1.78s/it]"
     ]
    }
   ],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "\n",
    "test_dataset = generate_qa_embedding_pairs(\n",
    "    llm=Settings.llm,\n",
    "    nodes=test_nodes,\n",
    "    output_path=\"ft_test_dataset.json\",\n",
    "    qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL,\n",
    "    num_questions_per_chunk=2,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = generate_qa_embedding_pairs(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    nodes=train_nodes,\n",
    "    output_path=\"train_dataset.json\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:45<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# qa_dataset_all_mini_eval = generate_question_context_pairs(nodes=nodes[n_nodes: n_nodes+50], num_questions_per_chunk=2, qa_generate_prompt_tmpl=QA_GENERATE_PROMPT_TMPL)\n",
    "# qa_dataset_all_mini_eval.save_json(\"../data/qa_dataset_all_mini_eval.json\")\n",
    "\n",
    "# qa_dataset_all_mini_eval = EmbeddingQAFinetuneDataset.from_json(\"../data/qa_dataset_all_mini_eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.finetuning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformersFinetuneEngine\n\u001b[1;32m      3\u001b[0m ft_engine \u001b[38;5;241m=\u001b[39m SentenceTransformersFinetuneEngine(\n\u001b[1;32m      4\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-mini-lm-l12-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model_output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_mini_lm_ft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     val_dataset\u001b[38;5;241m=\u001b[39mqa_dataset_all_mini_eval,\n\u001b[1;32m      7\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.finetuning'"
     ]
    }
   ],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "ft_engine = SentenceTransformersFinetuneEngine(\n",
    "    model_id=\"all-mini-lm-l12-v2\",\n",
    "    model_output_path=\"test_mini_lm_ft\",\n",
    "    val_dataset=qa_dataset_all_mini_eval,\n",
    "    num_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rust-rag-l9LsLaFj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
